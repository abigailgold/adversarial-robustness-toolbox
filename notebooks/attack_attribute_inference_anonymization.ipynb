{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running attribute inference attacks on regular and anonymized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will show how to run both black-box and white-box inference attacks. This will be demonstarted on the Nursery dataset (original dataset can be found here: https://archive.ics.uci.edu/ml/datasets/nursery). \n",
    "\n",
    "The sensitive feature we are trying to infer is the 'social' feature, after we have turned it into a binary feature (the original value 'problematic' receives the new value 1 and the rest 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>children</th>\n",
       "      <th>social</th>\n",
       "      <th>parents_pretentious</th>\n",
       "      <th>parents_great_pret</th>\n",
       "      <th>parents_usual</th>\n",
       "      <th>has_nurs_very_crit</th>\n",
       "      <th>has_nurs_improper</th>\n",
       "      <th>has_nurs_proper</th>\n",
       "      <th>has_nurs_critical</th>\n",
       "      <th>...</th>\n",
       "      <th>form_incomplete</th>\n",
       "      <th>form_foster</th>\n",
       "      <th>housing_critical</th>\n",
       "      <th>housing_convenient</th>\n",
       "      <th>housing_less_conv</th>\n",
       "      <th>finance_convenient</th>\n",
       "      <th>finance_inconv</th>\n",
       "      <th>health_priority</th>\n",
       "      <th>health_recommended</th>\n",
       "      <th>health_not_recom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>-0.704142</td>\n",
       "      <td>1.434509</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>2.000724</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.567324</td>\n",
       "      <td>-0.577425</td>\n",
       "      <td>1.428869</td>\n",
       "      <td>-0.711511</td>\n",
       "      <td>-0.709974</td>\n",
       "      <td>0.985252</td>\n",
       "      <td>-0.985252</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>-0.704142</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>1.402427</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>2.003141</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.567324</td>\n",
       "      <td>-0.577425</td>\n",
       "      <td>-0.699854</td>\n",
       "      <td>1.405459</td>\n",
       "      <td>-0.709974</td>\n",
       "      <td>-1.014968</td>\n",
       "      <td>1.014968</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.335242</td>\n",
       "      <td>1.420169</td>\n",
       "      <td>1.434509</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>1.997111</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.567324</td>\n",
       "      <td>-0.577425</td>\n",
       "      <td>-0.699854</td>\n",
       "      <td>-0.711511</td>\n",
       "      <td>1.408503</td>\n",
       "      <td>-1.014968</td>\n",
       "      <td>1.014968</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>-0.704142</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>1.978079</td>\n",
       "      <td>...</td>\n",
       "      <td>1.762661</td>\n",
       "      <td>-0.577425</td>\n",
       "      <td>-0.699854</td>\n",
       "      <td>1.405459</td>\n",
       "      <td>-0.709974</td>\n",
       "      <td>0.985252</td>\n",
       "      <td>-0.985252</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>-0.704142</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>1.997111</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.567324</td>\n",
       "      <td>-0.577425</td>\n",
       "      <td>1.428869</td>\n",
       "      <td>-0.711511</td>\n",
       "      <td>-0.709974</td>\n",
       "      <td>0.985252</td>\n",
       "      <td>-0.985252</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.337132</td>\n",
       "      <td>-0.704142</td>\n",
       "      <td>1.434509</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>1.978079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.567324</td>\n",
       "      <td>-0.577425</td>\n",
       "      <td>-0.699854</td>\n",
       "      <td>-0.711511</td>\n",
       "      <td>1.408503</td>\n",
       "      <td>-1.014968</td>\n",
       "      <td>1.014968</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>1.432625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.337132</td>\n",
       "      <td>1.420169</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>1.402427</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>2.003141</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>1.762661</td>\n",
       "      <td>-0.577425</td>\n",
       "      <td>-0.699854</td>\n",
       "      <td>1.405459</td>\n",
       "      <td>-0.709974</td>\n",
       "      <td>-1.014968</td>\n",
       "      <td>1.014968</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.446341</td>\n",
       "      <td>1.420169</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>2.003141</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>1.762661</td>\n",
       "      <td>-0.577425</td>\n",
       "      <td>-0.699854</td>\n",
       "      <td>1.405459</td>\n",
       "      <td>-0.709974</td>\n",
       "      <td>-1.014968</td>\n",
       "      <td>1.014968</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.446341</td>\n",
       "      <td>-0.704142</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>2.000724</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.567324</td>\n",
       "      <td>-0.577425</td>\n",
       "      <td>1.428869</td>\n",
       "      <td>-0.711511</td>\n",
       "      <td>-0.709974</td>\n",
       "      <td>0.985252</td>\n",
       "      <td>-0.985252</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5182</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.337132</td>\n",
       "      <td>1.420169</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>1.997111</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>1.762661</td>\n",
       "      <td>-0.577425</td>\n",
       "      <td>-0.699854</td>\n",
       "      <td>1.405459</td>\n",
       "      <td>-0.709974</td>\n",
       "      <td>0.985252</td>\n",
       "      <td>-0.985252</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5183 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  children    social  parents_pretentious  parents_great_pret  \\\n",
       "0         1  0.444450 -0.704142             1.434509           -0.713050   \n",
       "1         1  0.444450 -0.704142            -0.697102            1.402427   \n",
       "2         3  1.335242  1.420169             1.434509           -0.713050   \n",
       "3         3  0.444450 -0.704142            -0.697102           -0.713050   \n",
       "4         3  0.444450 -0.704142            -0.697102           -0.713050   \n",
       "...     ...       ...       ...                  ...                 ...   \n",
       "5178      0 -1.337132 -0.704142             1.434509           -0.713050   \n",
       "5179      1 -1.337132  1.420169            -0.697102            1.402427   \n",
       "5180      3 -0.446341  1.420169            -0.697102           -0.713050   \n",
       "5181      1 -0.446341 -0.704142            -0.697102           -0.713050   \n",
       "5182      3 -1.337132  1.420169            -0.697102           -0.713050   \n",
       "\n",
       "      parents_usual  has_nurs_very_crit  has_nurs_improper  has_nurs_proper  \\\n",
       "0         -0.711204            2.000724          -0.499216        -0.500723   \n",
       "1         -0.711204           -0.499819           2.003141        -0.500723   \n",
       "2         -0.711204           -0.499819          -0.499216         1.997111   \n",
       "3          1.406067           -0.499819          -0.499216        -0.500723   \n",
       "4          1.406067           -0.499819          -0.499216         1.997111   \n",
       "...             ...                 ...                ...              ...   \n",
       "5178      -0.711204           -0.499819          -0.499216        -0.500723   \n",
       "5179      -0.711204           -0.499819           2.003141        -0.500723   \n",
       "5180       1.406067           -0.499819           2.003141        -0.500723   \n",
       "5181       1.406067            2.000724          -0.499216        -0.500723   \n",
       "5182       1.406067           -0.499819          -0.499216         1.997111   \n",
       "\n",
       "      has_nurs_critical  ...  form_incomplete  form_foster  housing_critical  \\\n",
       "0             -0.505541  ...        -0.567324    -0.577425          1.428869   \n",
       "1             -0.505541  ...        -0.567324    -0.577425         -0.699854   \n",
       "2             -0.505541  ...        -0.567324    -0.577425         -0.699854   \n",
       "3              1.978079  ...         1.762661    -0.577425         -0.699854   \n",
       "4             -0.505541  ...        -0.567324    -0.577425          1.428869   \n",
       "...                 ...  ...              ...          ...               ...   \n",
       "5178           1.978079  ...        -0.567324    -0.577425         -0.699854   \n",
       "5179          -0.505541  ...         1.762661    -0.577425         -0.699854   \n",
       "5180          -0.505541  ...         1.762661    -0.577425         -0.699854   \n",
       "5181          -0.505541  ...        -0.567324    -0.577425          1.428869   \n",
       "5182          -0.505541  ...         1.762661    -0.577425         -0.699854   \n",
       "\n",
       "      housing_convenient  housing_less_conv  finance_convenient  \\\n",
       "0              -0.711511          -0.709974            0.985252   \n",
       "1               1.405459          -0.709974           -1.014968   \n",
       "2              -0.711511           1.408503           -1.014968   \n",
       "3               1.405459          -0.709974            0.985252   \n",
       "4              -0.711511          -0.709974            0.985252   \n",
       "...                  ...                ...                 ...   \n",
       "5178           -0.711511           1.408503           -1.014968   \n",
       "5179            1.405459          -0.709974           -1.014968   \n",
       "5180            1.405459          -0.709974           -1.014968   \n",
       "5181           -0.711511          -0.709974            0.985252   \n",
       "5182            1.405459          -0.709974            0.985252   \n",
       "\n",
       "      finance_inconv  health_priority  health_recommended  health_not_recom  \n",
       "0          -0.985252         1.399405           -0.708745         -0.698019  \n",
       "1           1.014968         1.399405           -0.708745         -0.698019  \n",
       "2           1.014968        -0.714590            1.410946         -0.698019  \n",
       "3          -0.985252         1.399405           -0.708745         -0.698019  \n",
       "4          -0.985252         1.399405           -0.708745         -0.698019  \n",
       "...              ...              ...                 ...               ...  \n",
       "5178        1.014968        -0.714590           -0.708745          1.432625  \n",
       "5179        1.014968        -0.714590            1.410946         -0.698019  \n",
       "5180        1.014968        -0.714590            1.410946         -0.698019  \n",
       "5181       -0.985252         1.399405           -0.708745         -0.698019  \n",
       "5182       -0.985252        -0.714590            1.410946         -0.698019  \n",
       "\n",
       "[5183 rows x 23 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Nursery_social_prepared_train.csv', sep=',', engine='python')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model accuracy:  0.6851851851851852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from art.estimators.classification.scikitlearn import ScikitlearnDecisionTreeClassifier\n",
    "\n",
    "features = df.drop(['label'], axis=1)\n",
    "labels = df.loc[:, 'label']\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(features, labels)\n",
    "\n",
    "art_classifier = ScikitlearnDecisionTreeClassifier(model)\n",
    "\n",
    "df_test = pd.read_csv('Nursery_prepared_test.csv', sep=',', engine='python')\n",
    "features_test = df_test.drop(['label'], axis=1)\n",
    "labels_test = df_test.loc[:, 'label']\n",
    "test_data = features_test.to_numpy()\n",
    "\n",
    "print('Base model accuracy: ', model.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack\n",
    "### Black-box attack\n",
    "The black-box attack basically trains an additional classifier (called the attack model) to predict the attacked feature's value from the remaining n-1 features as well as the original (attacked) model's predictions.\n",
    "#### Train attack model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from art.attacks.inference import AttributeInferenceBlackBox\n",
    "\n",
    "attack_feature = 1\n",
    "data = features.to_numpy()\n",
    "\n",
    "# training data without attacked feature\n",
    "x_train_for_attack = np.delete(data, attack_feature, 1)\n",
    "# only attacked feature\n",
    "x_train_feature = data[:, attack_feature].copy().reshape(-1, 1)\n",
    "\n",
    "bb_attack = AttributeInferenceBlackBox(art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get original model's predictions\n",
    "x_train_predictions = np.array([np.argmax(arr) for arr in art_classifier.predict(data)]).reshape(-1,1)\n",
    "\n",
    "# train attack model\n",
    "bb_attack.fit(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infer sensitive feature and check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6158595408064828\n"
     ]
    }
   ],
   "source": [
    "# get inferred values\n",
    "values = [-0.704141531, 1.420169037]\n",
    "inferred_train_bb = bb_attack.infer(x_train_for_attack, x_train_predictions, values=values)\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_bb == x_train_feature.reshape(1,-1)) / len(inferred_train_bb)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that for 61% of the training set, the attacked feature is inferred correctly using this attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitebox attacks\n",
    "These two attacks do not train any additional model, they simply use additional information coded within the attacked decision tree model to compute the probability of each value of the attacked feature and outputs the value with the highest probability.\n",
    "### First attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6183677406907196\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.inference import AttributeInferenceWhiteBoxLifestyleDecisionTree\n",
    "\n",
    "wb_attack = AttributeInferenceWhiteBoxLifestyleDecisionTree(art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "priors = [3465 / 5183, 1718 / 5183]\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_wb1 = wb_attack.infer(x_train_for_attack, x_train_predictions, values=values, priors=priors)\n",
    "\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_wb1 == x_train_feature.reshape(1,-1)) / len(inferred_train_wb1)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6905267219756898\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.inference import AttributeInferenceWhiteBoxDecisionTree\n",
    "\n",
    "wb2_attack = AttributeInferenceWhiteBoxDecisionTree(art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_wb2 = wb2_attack.infer(x_train_for_attack, x_train_predictions, values=values, priors=priors)\n",
    "\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_wb2 == x_train_feature.reshape(1,-1)) / len(inferred_train_wb2)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The white-box attacks are able to correctly infer the attacked feature value in 61% and 69% of the test set respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymized data\n",
    "## k=100\n",
    "\n",
    "Now we will apply the same attacks on an anonymized version of the same dataset (k=100). The data has been anonymized on the quasi-identifiers: form, housing, finance, social, health.\n",
    "\n",
    "k=100 means that each record in the anonymized dataset is identical to 99 others on the quasi-identifier values (i.e., when looking only at those 4 features, the records are indistinguishable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>children</th>\n",
       "      <th>social</th>\n",
       "      <th>parents_pretentious</th>\n",
       "      <th>parents_great_pret</th>\n",
       "      <th>parents_usual</th>\n",
       "      <th>has_nurs_very_crit</th>\n",
       "      <th>has_nurs_improper</th>\n",
       "      <th>has_nurs_proper</th>\n",
       "      <th>has_nurs_critical</th>\n",
       "      <th>...</th>\n",
       "      <th>form_foster</th>\n",
       "      <th>form_complete</th>\n",
       "      <th>housing_critical</th>\n",
       "      <th>housing_convenient</th>\n",
       "      <th>housing_less_conv</th>\n",
       "      <th>finance_convenient</th>\n",
       "      <th>finance_inconv</th>\n",
       "      <th>health_priority</th>\n",
       "      <th>health_recommended</th>\n",
       "      <th>health_not_recom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>-0.403596</td>\n",
       "      <td>1.434509</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>2.000724</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464554</td>\n",
       "      <td>-0.952322</td>\n",
       "      <td>0.942423</td>\n",
       "      <td>-0.536975</td>\n",
       "      <td>-0.572078</td>\n",
       "      <td>0.680053</td>\n",
       "      <td>-0.680053</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>-0.403596</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>1.402427</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>2.003141</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464554</td>\n",
       "      <td>-0.952322</td>\n",
       "      <td>-1.061095</td>\n",
       "      <td>1.862284</td>\n",
       "      <td>-0.572078</td>\n",
       "      <td>-1.470474</td>\n",
       "      <td>1.470474</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.335242</td>\n",
       "      <td>2.477724</td>\n",
       "      <td>1.434509</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>1.997111</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464554</td>\n",
       "      <td>-0.952322</td>\n",
       "      <td>-1.061095</td>\n",
       "      <td>-0.536975</td>\n",
       "      <td>1.748015</td>\n",
       "      <td>0.680053</td>\n",
       "      <td>-0.680053</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>-0.403596</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>1.978079</td>\n",
       "      <td>...</td>\n",
       "      <td>2.152602</td>\n",
       "      <td>-0.952322</td>\n",
       "      <td>-1.061095</td>\n",
       "      <td>1.862284</td>\n",
       "      <td>-0.572078</td>\n",
       "      <td>0.680053</td>\n",
       "      <td>-0.680053</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>-0.403596</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>1.997111</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464554</td>\n",
       "      <td>1.050065</td>\n",
       "      <td>0.942423</td>\n",
       "      <td>-0.536975</td>\n",
       "      <td>-0.572078</td>\n",
       "      <td>0.680053</td>\n",
       "      <td>-0.680053</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.337132</td>\n",
       "      <td>-0.403596</td>\n",
       "      <td>1.434509</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>1.978079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464554</td>\n",
       "      <td>1.050065</td>\n",
       "      <td>0.942423</td>\n",
       "      <td>-0.536975</td>\n",
       "      <td>-0.572078</td>\n",
       "      <td>0.680053</td>\n",
       "      <td>-0.680053</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>1.432625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.337132</td>\n",
       "      <td>2.477724</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>1.402427</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>2.003141</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464554</td>\n",
       "      <td>1.050065</td>\n",
       "      <td>-1.061095</td>\n",
       "      <td>1.862284</td>\n",
       "      <td>-0.572078</td>\n",
       "      <td>-1.470474</td>\n",
       "      <td>1.470474</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.446341</td>\n",
       "      <td>2.477724</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>2.003141</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464554</td>\n",
       "      <td>1.050065</td>\n",
       "      <td>-1.061095</td>\n",
       "      <td>1.862284</td>\n",
       "      <td>-0.572078</td>\n",
       "      <td>-1.470474</td>\n",
       "      <td>1.470474</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.446341</td>\n",
       "      <td>-0.403596</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>2.000724</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464554</td>\n",
       "      <td>1.050065</td>\n",
       "      <td>0.942423</td>\n",
       "      <td>-0.536975</td>\n",
       "      <td>-0.572078</td>\n",
       "      <td>0.680053</td>\n",
       "      <td>-0.680053</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5182</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.337132</td>\n",
       "      <td>2.477724</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>1.997111</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.464554</td>\n",
       "      <td>1.050065</td>\n",
       "      <td>-1.061095</td>\n",
       "      <td>1.862284</td>\n",
       "      <td>-0.572078</td>\n",
       "      <td>-1.470474</td>\n",
       "      <td>1.470474</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5183 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  children    social  parents_pretentious  parents_great_pret  \\\n",
       "0         1  0.444450 -0.403596             1.434509           -0.713050   \n",
       "1         1  0.444450 -0.403596            -0.697102            1.402427   \n",
       "2         3  1.335242  2.477724             1.434509           -0.713050   \n",
       "3         3  0.444450 -0.403596            -0.697102           -0.713050   \n",
       "4         3  0.444450 -0.403596            -0.697102           -0.713050   \n",
       "...     ...       ...       ...                  ...                 ...   \n",
       "5178      0 -1.337132 -0.403596             1.434509           -0.713050   \n",
       "5179      1 -1.337132  2.477724            -0.697102            1.402427   \n",
       "5180      3 -0.446341  2.477724            -0.697102           -0.713050   \n",
       "5181      1 -0.446341 -0.403596            -0.697102           -0.713050   \n",
       "5182      3 -1.337132  2.477724            -0.697102           -0.713050   \n",
       "\n",
       "      parents_usual  has_nurs_very_crit  has_nurs_improper  has_nurs_proper  \\\n",
       "0         -0.711204            2.000724          -0.499216        -0.500723   \n",
       "1         -0.711204           -0.499819           2.003141        -0.500723   \n",
       "2         -0.711204           -0.499819          -0.499216         1.997111   \n",
       "3          1.406067           -0.499819          -0.499216        -0.500723   \n",
       "4          1.406067           -0.499819          -0.499216         1.997111   \n",
       "...             ...                 ...                ...              ...   \n",
       "5178      -0.711204           -0.499819          -0.499216        -0.500723   \n",
       "5179      -0.711204           -0.499819           2.003141        -0.500723   \n",
       "5180       1.406067           -0.499819           2.003141        -0.500723   \n",
       "5181       1.406067            2.000724          -0.499216        -0.500723   \n",
       "5182       1.406067           -0.499819          -0.499216         1.997111   \n",
       "\n",
       "      has_nurs_critical  ...  form_foster  form_complete  housing_critical  \\\n",
       "0             -0.505541  ...    -0.464554      -0.952322          0.942423   \n",
       "1             -0.505541  ...    -0.464554      -0.952322         -1.061095   \n",
       "2             -0.505541  ...    -0.464554      -0.952322         -1.061095   \n",
       "3              1.978079  ...     2.152602      -0.952322         -1.061095   \n",
       "4             -0.505541  ...    -0.464554       1.050065          0.942423   \n",
       "...                 ...  ...          ...            ...               ...   \n",
       "5178           1.978079  ...    -0.464554       1.050065          0.942423   \n",
       "5179          -0.505541  ...    -0.464554       1.050065         -1.061095   \n",
       "5180          -0.505541  ...    -0.464554       1.050065         -1.061095   \n",
       "5181          -0.505541  ...    -0.464554       1.050065          0.942423   \n",
       "5182          -0.505541  ...    -0.464554       1.050065         -1.061095   \n",
       "\n",
       "      housing_convenient  housing_less_conv  finance_convenient  \\\n",
       "0              -0.536975          -0.572078            0.680053   \n",
       "1               1.862284          -0.572078           -1.470474   \n",
       "2              -0.536975           1.748015            0.680053   \n",
       "3               1.862284          -0.572078            0.680053   \n",
       "4              -0.536975          -0.572078            0.680053   \n",
       "...                  ...                ...                 ...   \n",
       "5178           -0.536975          -0.572078            0.680053   \n",
       "5179            1.862284          -0.572078           -1.470474   \n",
       "5180            1.862284          -0.572078           -1.470474   \n",
       "5181           -0.536975          -0.572078            0.680053   \n",
       "5182            1.862284          -0.572078           -1.470474   \n",
       "\n",
       "      finance_inconv  health_priority  health_recommended  health_not_recom  \n",
       "0          -0.680053         1.399405           -0.708745         -0.698019  \n",
       "1           1.470474         1.399405           -0.708745         -0.698019  \n",
       "2          -0.680053        -0.714590            1.410946         -0.698019  \n",
       "3          -0.680053         1.399405           -0.708745         -0.698019  \n",
       "4          -0.680053         1.399405           -0.708745         -0.698019  \n",
       "...              ...              ...                 ...               ...  \n",
       "5178       -0.680053        -0.714590           -0.708745          1.432625  \n",
       "5179        1.470474        -0.714590            1.410946         -0.698019  \n",
       "5180        1.470474        -0.714590            1.410946         -0.698019  \n",
       "5181       -0.680053         1.399405           -0.708745         -0.698019  \n",
       "5182        1.470474        -0.714590            1.410946         -0.698019  \n",
       "\n",
       "[5183 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anon_df = pd.read_csv('Nursery_anonymized_prepared_train.csv', sep=',', engine='python')\n",
    "\n",
    "anon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4479"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct rows in original data\n",
    "len(df.drop_duplicates().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct rows in anonymized data\n",
    "len(anon_df.drop_duplicates().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized model accuracy:  0.6304012345679012\n"
     ]
    }
   ],
   "source": [
    "anon_features = anon_df.drop(['label'], axis=1)\n",
    "anon_labels = anon_df.loc[:, 'label']\n",
    "anon_model = DecisionTreeClassifier()\n",
    "anon_model.fit(anon_features, anon_labels)\n",
    "\n",
    "anon_art_classifier = ScikitlearnDecisionTreeClassifier(anon_model)\n",
    "\n",
    "print('Anonymized model accuracy: ', anon_model.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack\n",
    "### Black-box attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6280146633224002\n"
     ]
    }
   ],
   "source": [
    "anon_bb_attack = AttributeInferenceBlackBox(anon_art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get original model's predictions\n",
    "anon_x_train_predictions = np.array([np.argmax(arr) for arr in anon_art_classifier.predict(data)]).reshape(-1,1)\n",
    "\n",
    "# train attack model\n",
    "anon_bb_attack.fit(test_data)\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_anon_bb = anon_bb_attack.infer(x_train_for_attack, anon_x_train_predictions, values=values)\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_anon_bb == x_train_feature.reshape(1,-1)) / len(inferred_train_anon_bb)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White box attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6143160331854138\n"
     ]
    }
   ],
   "source": [
    "anon_wb_attack = AttributeInferenceWhiteBoxLifestyleDecisionTree(anon_art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_anon_wb1 = anon_wb_attack.infer(x_train_for_attack, anon_x_train_predictions, values=values, priors=priors)\n",
    "\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_anon_wb1 == x_train_feature.reshape(1,-1)) / len(inferred_train_anon_wb1)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6955431217441637\n"
     ]
    }
   ],
   "source": [
    "anon_wb2_attack = AttributeInferenceWhiteBoxDecisionTree(anon_art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_anon_wb2 = anon_wb2_attack.infer(x_train_for_attack, anon_x_train_predictions, values=values, priors=priors)\n",
    "\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_anon_wb2 == x_train_feature.reshape(1,-1)) / len(inferred_train_anon_wb2)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the attacks remains more or less the same. This may be due to the fact that the prior distribution of the larger class has increased (from 0.67 to 0.85). Let's check the precision and recall for each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4103703703703704, 0.3224679860302678)\n",
      "(0.41613418530351437, 0.3032596041909197)\n"
     ]
    }
   ],
   "source": [
    "def calc_precision_recall(predicted, actual, positive_value=1):\n",
    "    score = 0  # both predicted and actual are positive\n",
    "    num_positive_predicted = 0  # predicted positive\n",
    "    num_positive_actual = 0  # actual positive\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] == positive_value:\n",
    "            num_positive_predicted += 1\n",
    "        if actual[i] == positive_value:\n",
    "            num_positive_actual += 1\n",
    "        if predicted[i] == actual[i]:\n",
    "            if predicted[i] == positive_value:\n",
    "                score += 1\n",
    "    \n",
    "    if num_positive_predicted == 0:\n",
    "        precision = 1\n",
    "    else:\n",
    "        precision = score / num_positive_predicted  # the fraction of predicted “Yes” responses that are correct\n",
    "    if num_positive_actual == 0:\n",
    "        recall = 1\n",
    "    else:\n",
    "        recall = score / num_positive_actual  # the fraction of “Yes” responses that are predicted correctly\n",
    "\n",
    "    return precision, recall\n",
    "    \n",
    "# black-box regular\n",
    "print(calc_precision_recall(inferred_train_bb, x_train_feature, positive_value=1.420169037))\n",
    "# black-box anonymized\n",
    "print(calc_precision_recall(inferred_train_anon_bb, x_train_feature, positive_value=1.420169037))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3402948402948403, 0.1612339930151339)\n",
      "(0.3426651735722284, 0.1781140861466822)\n",
      "(0.5880551301684533, 0.22351571594877764)\n",
      "(0.612540192926045, 0.2217694994179278)\n"
     ]
    }
   ],
   "source": [
    "# white-box 1 regular\n",
    "print(calc_precision_recall(inferred_train_wb1, x_train_feature, positive_value=1.420169037))\n",
    "# white-box 1 anonymized\n",
    "print(calc_precision_recall(inferred_train_anon_wb1, x_train_feature, positive_value=1.420169037))\n",
    "\n",
    "# white-box 2 regular\n",
    "print(calc_precision_recall(inferred_train_wb2, x_train_feature, positive_value=1.420169037))\n",
    "# white-box 2 anonymized\n",
    "print(calc_precision_recall(inferred_train_anon_wb2, x_train_feature, positive_value=1.420169037))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall remain almost the same, sometimes even slightly increasing.\n",
    "\n",
    "Now let's see what happens when we increase k to 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k=1000\n",
    "\n",
    "Now we apply the attacks on an anonymized version of the same dataset (k=1000). The data has been anonymized on the quasi-identifiers: form, housing, finance, social, health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>children</th>\n",
       "      <th>social</th>\n",
       "      <th>parents_pretentious</th>\n",
       "      <th>parents_great_pret</th>\n",
       "      <th>parents_usual</th>\n",
       "      <th>has_nurs_very_crit</th>\n",
       "      <th>has_nurs_improper</th>\n",
       "      <th>has_nurs_proper</th>\n",
       "      <th>has_nurs_critical</th>\n",
       "      <th>...</th>\n",
       "      <th>form_completed.1</th>\n",
       "      <th>form_complete</th>\n",
       "      <th>housing_convenient</th>\n",
       "      <th>housing_less_conv</th>\n",
       "      <th>housing_critical</th>\n",
       "      <th>finance_inconv</th>\n",
       "      <th>finance_convenient</th>\n",
       "      <th>health_priority</th>\n",
       "      <th>health_recommended</th>\n",
       "      <th>health_not_recom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>0</td>\n",
       "      <td>1.434509</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>2.000724</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-1.399405</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>1.402427</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>2.003141</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-1.399405</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.335242</td>\n",
       "      <td>0</td>\n",
       "      <td>1.434509</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>1.997111</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>0.714590</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>1.978079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-1.399405</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>1.997111</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-1.399405</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.337132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.434509</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>1.978079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>1.432625</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>1.432625</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>0.714590</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>1.432625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.337132</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>1.402427</td>\n",
       "      <td>-0.711204</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>2.003141</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>0.714590</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.446341</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>2.003141</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>0.714590</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.446341</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>2.000724</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-1.399405</td>\n",
       "      <td>1.399405</td>\n",
       "      <td>-0.708745</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5182</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.337132</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.697102</td>\n",
       "      <td>-0.713050</td>\n",
       "      <td>1.406067</td>\n",
       "      <td>-0.499819</td>\n",
       "      <td>-0.499216</td>\n",
       "      <td>1.997111</td>\n",
       "      <td>-0.505541</td>\n",
       "      <td>...</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>0.714590</td>\n",
       "      <td>-0.714590</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5183 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  children  social  parents_pretentious  parents_great_pret  \\\n",
       "0         4  0.444450       0             1.434509           -0.713050   \n",
       "1         4  0.444450       0            -0.697102            1.402427   \n",
       "2         3  1.335242       0             1.434509           -0.713050   \n",
       "3         3  0.444450       0            -0.697102           -0.713050   \n",
       "4         3  0.444450       0            -0.697102           -0.713050   \n",
       "...     ...       ...     ...                  ...                 ...   \n",
       "5178      0 -1.337132       0             1.434509           -0.713050   \n",
       "5179      4 -1.337132       0            -0.697102            1.402427   \n",
       "5180      3 -0.446341       0            -0.697102           -0.713050   \n",
       "5181      4 -0.446341       0            -0.697102           -0.713050   \n",
       "5182      3 -1.337132       0            -0.697102           -0.713050   \n",
       "\n",
       "      parents_usual  has_nurs_very_crit  has_nurs_improper  has_nurs_proper  \\\n",
       "0         -0.711204            2.000724          -0.499216        -0.500723   \n",
       "1         -0.711204           -0.499819           2.003141        -0.500723   \n",
       "2         -0.711204           -0.499819          -0.499216         1.997111   \n",
       "3          1.406067           -0.499819          -0.499216        -0.500723   \n",
       "4          1.406067           -0.499819          -0.499216         1.997111   \n",
       "...             ...                 ...                ...              ...   \n",
       "5178      -0.711204           -0.499819          -0.499216        -0.500723   \n",
       "5179      -0.711204           -0.499819           2.003141        -0.500723   \n",
       "5180       1.406067           -0.499819           2.003141        -0.500723   \n",
       "5181       1.406067            2.000724          -0.499216        -0.500723   \n",
       "5182       1.406067           -0.499819          -0.499216         1.997111   \n",
       "\n",
       "      has_nurs_critical  ...  form_completed.1  form_complete  \\\n",
       "0             -0.505541  ...         -0.708745      -0.698019   \n",
       "1             -0.505541  ...         -0.708745      -0.698019   \n",
       "2             -0.505541  ...          1.410946      -0.698019   \n",
       "3              1.978079  ...         -0.708745      -0.698019   \n",
       "4             -0.505541  ...         -0.708745      -0.698019   \n",
       "...                 ...  ...               ...            ...   \n",
       "5178           1.978079  ...         -0.708745       1.432625   \n",
       "5179          -0.505541  ...          1.410946      -0.698019   \n",
       "5180          -0.505541  ...          1.410946      -0.698019   \n",
       "5181          -0.505541  ...         -0.708745      -0.698019   \n",
       "5182          -0.505541  ...          1.410946      -0.698019   \n",
       "\n",
       "      housing_convenient  housing_less_conv  housing_critical  finance_inconv  \\\n",
       "0               1.399405          -0.708745         -0.698019        1.399405   \n",
       "1               1.399405          -0.708745         -0.698019        1.399405   \n",
       "2              -0.714590           1.410946         -0.698019       -0.714590   \n",
       "3               1.399405          -0.708745         -0.698019        1.399405   \n",
       "4               1.399405          -0.708745         -0.698019        1.399405   \n",
       "...                  ...                ...               ...             ...   \n",
       "5178           -0.714590          -0.708745          1.432625       -0.714590   \n",
       "5179           -0.714590           1.410946         -0.698019       -0.714590   \n",
       "5180           -0.714590           1.410946         -0.698019       -0.714590   \n",
       "5181            1.399405          -0.708745         -0.698019        1.399405   \n",
       "5182           -0.714590           1.410946         -0.698019       -0.714590   \n",
       "\n",
       "      finance_convenient  health_priority  health_recommended  \\\n",
       "0              -1.399405         1.399405           -0.708745   \n",
       "1              -1.399405         1.399405           -0.708745   \n",
       "2               0.714590        -0.714590            1.410946   \n",
       "3              -1.399405         1.399405           -0.708745   \n",
       "4              -1.399405         1.399405           -0.708745   \n",
       "...                  ...              ...                 ...   \n",
       "5178            0.714590        -0.714590           -0.708745   \n",
       "5179            0.714590        -0.714590            1.410946   \n",
       "5180            0.714590        -0.714590            1.410946   \n",
       "5181           -1.399405         1.399405           -0.708745   \n",
       "5182            0.714590        -0.714590            1.410946   \n",
       "\n",
       "      health_not_recom  \n",
       "0            -0.698019  \n",
       "1            -0.698019  \n",
       "2            -0.698019  \n",
       "3            -0.698019  \n",
       "4            -0.698019  \n",
       "...                ...  \n",
       "5178          1.432625  \n",
       "5179         -0.698019  \n",
       "5180         -0.698019  \n",
       "5181         -0.698019  \n",
       "5182         -0.698019  \n",
       "\n",
       "[5183 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anon2_df = pd.read_csv('Nursery_anonymized1000_prepared_train.csv', sep=',', engine='python')\n",
    "\n",
    "anon2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct rows in anonymized data\n",
    "len(anon2_df.drop_duplicates().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized model accuracy:  0.8742283950617284\n"
     ]
    }
   ],
   "source": [
    "anon2_features = anon2_df.drop(['label'], axis=1)\n",
    "anon2_labels = anon2_df.loc[:, 'label']\n",
    "anon2_model = DecisionTreeClassifier()\n",
    "anon2_model.fit(anon2_features, anon2_labels)\n",
    "\n",
    "anon2_art_classifier = ScikitlearnDecisionTreeClassifier(anon2_model)\n",
    "\n",
    "print('Anonymized model accuracy: ', anon2_model.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack\n",
    "### Black-box attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.585568203743006\n"
     ]
    }
   ],
   "source": [
    "anon2_bb_attack = AttributeInferenceBlackBox(anon2_art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get original model's predictions\n",
    "anon2_x_train_predictions = np.array([np.argmax(arr) for arr in anon2_art_classifier.predict(data)]).reshape(-1,1)\n",
    "\n",
    "# train attack model\n",
    "anon2_bb_attack.fit(test_data)\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_anon2_bb = anon2_bb_attack.infer(x_train_for_attack, anon2_x_train_predictions, values=values)\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_anon2_bb == x_train_feature.reshape(1,-1)) / len(inferred_train_anon2_bb)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White box attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6685317383754582\n"
     ]
    }
   ],
   "source": [
    "anon2_wb_attack = AttributeInferenceWhiteBoxLifestyleDecisionTree(anon2_art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_anon2_wb1 = anon2_wb_attack.infer(x_train_for_attack, anon2_x_train_predictions, values=values, priors=priors)\n",
    "\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_anon2_wb1 == x_train_feature.reshape(1,-1)) / len(inferred_train_anon2_wb1)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6685317383754582\n"
     ]
    }
   ],
   "source": [
    "anon2_wb2_attack = AttributeInferenceWhiteBoxDecisionTree(anon2_art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_anon2_wb2 = anon2_wb2_attack.infer(x_train_for_attack, anon2_x_train_predictions, values=values, priors=priors)\n",
    "\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_anon2_wb2 == x_train_feature.reshape(1,-1)) / len(inferred_train_anon_wb2)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the accuracy of the attacks has slightly increased. Let's check the precision and recall for each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4103703703703704, 0.3224679860302678)\n",
      "(0.3348694316436252, 0.25378346915017463)\n",
      "(0.3402948402948403, 0.1612339930151339)\n",
      "(1, 0.0)\n",
      "(0.5880551301684533, 0.22351571594877764)\n",
      "(1, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# black-box regular\n",
    "print(calc_precision_recall(inferred_train_bb, x_train_feature, positive_value=1.420169037))\n",
    "# black-box anonymized\n",
    "print(calc_precision_recall(inferred_train_anon2_bb, x_train_feature, positive_value=1.420169037))\n",
    "\n",
    "# white-box 1 regular\n",
    "print(calc_precision_recall(inferred_train_wb1, x_train_feature, positive_value=1.420169037))\n",
    "# white-box 1 anonymized\n",
    "print(calc_precision_recall(inferred_train_anon2_wb1, x_train_feature, positive_value=1.420169037))\n",
    "\n",
    "# white-box 2 regular\n",
    "print(calc_precision_recall(inferred_train_wb2, x_train_feature, positive_value=1.420169037))\n",
    "# white-box 2 anonymized\n",
    "print(calc_precision_recall(inferred_train_anon2_wb2, x_train_feature, positive_value=1.420169037))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall decreased in all cases.\n",
    "\n",
    "*In the anonymized version of the white-box attacks, no records were predicted with the positive value for the attacked feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k=100, all QI\n",
    "Now let's see what happens if we define all 8 features in the Nursery dataset as quasi-identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>children</th>\n",
       "      <th>social</th>\n",
       "      <th>parents_pretentious</th>\n",
       "      <th>parents_great_pret</th>\n",
       "      <th>parents_usual</th>\n",
       "      <th>has_nurs_very_crit</th>\n",
       "      <th>has_nurs_improper</th>\n",
       "      <th>has_nurs_less_proper</th>\n",
       "      <th>has_nurs_critical</th>\n",
       "      <th>...</th>\n",
       "      <th>form_foster</th>\n",
       "      <th>form_incomplete</th>\n",
       "      <th>housing_critical</th>\n",
       "      <th>housing_less_conv</th>\n",
       "      <th>housing_convenient</th>\n",
       "      <th>finance_convenient</th>\n",
       "      <th>finance_inconv</th>\n",
       "      <th>health_priority</th>\n",
       "      <th>health_recommended</th>\n",
       "      <th>health_not_recom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2.209469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.463928</td>\n",
       "      <td>-1.184900</td>\n",
       "      <td>-0.615189</td>\n",
       "      <td>1.074753</td>\n",
       "      <td>-0.388929</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.396783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479224</td>\n",
       "      <td>-0.440726</td>\n",
       "      <td>1.059043</td>\n",
       "      <td>-0.767083</td>\n",
       "      <td>-0.433525</td>\n",
       "      <td>0.618467</td>\n",
       "      <td>-0.618467</td>\n",
       "      <td>1.389210</td>\n",
       "      <td>-0.703529</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.535492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.405856</td>\n",
       "      <td>0.843953</td>\n",
       "      <td>-0.615189</td>\n",
       "      <td>-0.930447</td>\n",
       "      <td>2.571161</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.396783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479224</td>\n",
       "      <td>-0.440726</td>\n",
       "      <td>-0.944249</td>\n",
       "      <td>1.303641</td>\n",
       "      <td>-0.433525</td>\n",
       "      <td>-1.616900</td>\n",
       "      <td>1.616900</td>\n",
       "      <td>1.389210</td>\n",
       "      <td>-0.703529</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.535492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.463928</td>\n",
       "      <td>-1.184900</td>\n",
       "      <td>-0.615189</td>\n",
       "      <td>-0.930447</td>\n",
       "      <td>-0.388929</td>\n",
       "      <td>1.778497</td>\n",
       "      <td>-0.396783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479224</td>\n",
       "      <td>-0.440726</td>\n",
       "      <td>-0.944249</td>\n",
       "      <td>1.303641</td>\n",
       "      <td>-0.433525</td>\n",
       "      <td>-1.616900</td>\n",
       "      <td>1.616900</td>\n",
       "      <td>-0.719834</td>\n",
       "      <td>1.421406</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.138485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.405856</td>\n",
       "      <td>-1.184900</td>\n",
       "      <td>1.625517</td>\n",
       "      <td>-0.930447</td>\n",
       "      <td>-0.388929</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>2.520272</td>\n",
       "      <td>...</td>\n",
       "      <td>2.086705</td>\n",
       "      <td>-0.440726</td>\n",
       "      <td>-0.944249</td>\n",
       "      <td>-0.767083</td>\n",
       "      <td>2.306671</td>\n",
       "      <td>0.618467</td>\n",
       "      <td>-0.618467</td>\n",
       "      <td>-0.719834</td>\n",
       "      <td>1.421406</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.138485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.405856</td>\n",
       "      <td>-1.184900</td>\n",
       "      <td>1.625517</td>\n",
       "      <td>-0.930447</td>\n",
       "      <td>-0.388929</td>\n",
       "      <td>1.778497</td>\n",
       "      <td>-0.396783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479224</td>\n",
       "      <td>-0.440726</td>\n",
       "      <td>-0.944249</td>\n",
       "      <td>1.303641</td>\n",
       "      <td>-0.433525</td>\n",
       "      <td>0.618467</td>\n",
       "      <td>-0.618467</td>\n",
       "      <td>1.389210</td>\n",
       "      <td>-0.703529</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0</td>\n",
       "      <td>0.535492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.405856</td>\n",
       "      <td>0.843953</td>\n",
       "      <td>-0.615189</td>\n",
       "      <td>1.074753</td>\n",
       "      <td>-0.388929</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.396783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479224</td>\n",
       "      <td>-0.440726</td>\n",
       "      <td>1.059043</td>\n",
       "      <td>-0.767083</td>\n",
       "      <td>-0.433525</td>\n",
       "      <td>0.618467</td>\n",
       "      <td>-0.618467</td>\n",
       "      <td>-0.719834</td>\n",
       "      <td>-0.703529</td>\n",
       "      <td>1.432625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>4</td>\n",
       "      <td>0.535492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.405856</td>\n",
       "      <td>0.843953</td>\n",
       "      <td>-0.615189</td>\n",
       "      <td>-0.930447</td>\n",
       "      <td>2.571161</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.396783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479224</td>\n",
       "      <td>-0.440726</td>\n",
       "      <td>-0.944249</td>\n",
       "      <td>1.303641</td>\n",
       "      <td>-0.433525</td>\n",
       "      <td>-1.616900</td>\n",
       "      <td>1.616900</td>\n",
       "      <td>1.389210</td>\n",
       "      <td>-0.703529</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.138485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.405856</td>\n",
       "      <td>-1.184900</td>\n",
       "      <td>1.625517</td>\n",
       "      <td>-0.930447</td>\n",
       "      <td>2.571161</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.396783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479224</td>\n",
       "      <td>2.268982</td>\n",
       "      <td>-0.944249</td>\n",
       "      <td>1.303641</td>\n",
       "      <td>-0.433525</td>\n",
       "      <td>0.618467</td>\n",
       "      <td>-0.618467</td>\n",
       "      <td>1.389210</td>\n",
       "      <td>-0.703529</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.138485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.405856</td>\n",
       "      <td>-1.184900</td>\n",
       "      <td>1.625517</td>\n",
       "      <td>1.074753</td>\n",
       "      <td>-0.388929</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.396783</td>\n",
       "      <td>...</td>\n",
       "      <td>2.086705</td>\n",
       "      <td>-0.440726</td>\n",
       "      <td>1.059043</td>\n",
       "      <td>-0.767083</td>\n",
       "      <td>-0.433525</td>\n",
       "      <td>0.618467</td>\n",
       "      <td>-0.618467</td>\n",
       "      <td>1.389210</td>\n",
       "      <td>-0.703529</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5182</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.138485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.405856</td>\n",
       "      <td>-1.184900</td>\n",
       "      <td>1.625517</td>\n",
       "      <td>-0.930447</td>\n",
       "      <td>-0.388929</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.396783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479224</td>\n",
       "      <td>-0.440726</td>\n",
       "      <td>-0.944249</td>\n",
       "      <td>-0.767083</td>\n",
       "      <td>2.306671</td>\n",
       "      <td>0.618467</td>\n",
       "      <td>-0.618467</td>\n",
       "      <td>-0.719834</td>\n",
       "      <td>1.421406</td>\n",
       "      <td>-0.698019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5183 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  children  social  parents_pretentious  parents_great_pret  \\\n",
       "0         4  2.209469     0.0             2.463928           -1.184900   \n",
       "1         4  0.535492     0.0            -0.405856            0.843953   \n",
       "2         3  0.535492     0.0             2.463928           -1.184900   \n",
       "3         3 -1.138485     0.0            -0.405856           -1.184900   \n",
       "4         3 -1.138485     0.0            -0.405856           -1.184900   \n",
       "...     ...       ...     ...                  ...                 ...   \n",
       "5178      0  0.535492     0.0            -0.405856            0.843953   \n",
       "5179      4  0.535492     0.0            -0.405856            0.843953   \n",
       "5180      3 -1.138485     0.0            -0.405856           -1.184900   \n",
       "5181      4 -1.138485     0.0            -0.405856           -1.184900   \n",
       "5182      3 -1.138485     0.0            -0.405856           -1.184900   \n",
       "\n",
       "      parents_usual  has_nurs_very_crit  has_nurs_improper  \\\n",
       "0         -0.615189            1.074753          -0.388929   \n",
       "1         -0.615189           -0.930447           2.571161   \n",
       "2         -0.615189           -0.930447          -0.388929   \n",
       "3          1.625517           -0.930447          -0.388929   \n",
       "4          1.625517           -0.930447          -0.388929   \n",
       "...             ...                 ...                ...   \n",
       "5178      -0.615189            1.074753          -0.388929   \n",
       "5179      -0.615189           -0.930447           2.571161   \n",
       "5180       1.625517           -0.930447           2.571161   \n",
       "5181       1.625517            1.074753          -0.388929   \n",
       "5182       1.625517           -0.930447          -0.388929   \n",
       "\n",
       "      has_nurs_less_proper  has_nurs_critical  ...  form_foster  \\\n",
       "0                -0.562272          -0.396783  ...    -0.479224   \n",
       "1                -0.562272          -0.396783  ...    -0.479224   \n",
       "2                 1.778497          -0.396783  ...    -0.479224   \n",
       "3                -0.562272           2.520272  ...     2.086705   \n",
       "4                 1.778497          -0.396783  ...    -0.479224   \n",
       "...                    ...                ...  ...          ...   \n",
       "5178             -0.562272          -0.396783  ...    -0.479224   \n",
       "5179             -0.562272          -0.396783  ...    -0.479224   \n",
       "5180             -0.562272          -0.396783  ...    -0.479224   \n",
       "5181             -0.562272          -0.396783  ...     2.086705   \n",
       "5182             -0.562272          -0.396783  ...    -0.479224   \n",
       "\n",
       "      form_incomplete  housing_critical  housing_less_conv  \\\n",
       "0           -0.440726          1.059043          -0.767083   \n",
       "1           -0.440726         -0.944249           1.303641   \n",
       "2           -0.440726         -0.944249           1.303641   \n",
       "3           -0.440726         -0.944249          -0.767083   \n",
       "4           -0.440726         -0.944249           1.303641   \n",
       "...               ...               ...                ...   \n",
       "5178        -0.440726          1.059043          -0.767083   \n",
       "5179        -0.440726         -0.944249           1.303641   \n",
       "5180         2.268982         -0.944249           1.303641   \n",
       "5181        -0.440726          1.059043          -0.767083   \n",
       "5182        -0.440726         -0.944249          -0.767083   \n",
       "\n",
       "      housing_convenient  finance_convenient  finance_inconv  health_priority  \\\n",
       "0              -0.433525            0.618467       -0.618467         1.389210   \n",
       "1              -0.433525           -1.616900        1.616900         1.389210   \n",
       "2              -0.433525           -1.616900        1.616900        -0.719834   \n",
       "3               2.306671            0.618467       -0.618467        -0.719834   \n",
       "4              -0.433525            0.618467       -0.618467         1.389210   \n",
       "...                  ...                 ...             ...              ...   \n",
       "5178           -0.433525            0.618467       -0.618467        -0.719834   \n",
       "5179           -0.433525           -1.616900        1.616900         1.389210   \n",
       "5180           -0.433525            0.618467       -0.618467         1.389210   \n",
       "5181           -0.433525            0.618467       -0.618467         1.389210   \n",
       "5182            2.306671            0.618467       -0.618467        -0.719834   \n",
       "\n",
       "      health_recommended  health_not_recom  \n",
       "0              -0.703529         -0.698019  \n",
       "1              -0.703529         -0.698019  \n",
       "2               1.421406         -0.698019  \n",
       "3               1.421406         -0.698019  \n",
       "4              -0.703529         -0.698019  \n",
       "...                  ...               ...  \n",
       "5178           -0.703529          1.432625  \n",
       "5179           -0.703529         -0.698019  \n",
       "5180           -0.703529         -0.698019  \n",
       "5181           -0.703529         -0.698019  \n",
       "5182            1.421406         -0.698019  \n",
       "\n",
       "[5183 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anon3_df = pd.read_csv('Nursery_anonymized_allQI_prepared_train.csv', sep=',', engine='python')\n",
    "\n",
    "anon3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct rows in anonymized data\n",
    "len(anon3_df.drop_duplicates().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized model accuracy:  0.7445987654320988\n",
      "0.5653096662164769\n",
      "0.6685317383754582\n",
      "0.6685317383754582\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "anon3_features = anon3_df.drop(['label'], axis=1)\n",
    "anon3_labels = anon3_df.loc[:, 'label']\n",
    "anon3_model = DecisionTreeClassifier()\n",
    "anon3_model.fit(anon3_features, anon3_labels)\n",
    "\n",
    "anon3_art_classifier = ScikitlearnDecisionTreeClassifier(anon3_model)\n",
    "\n",
    "print('Anonymized model accuracy: ', anon3_model.score(features_test, labels_test))\n",
    "\n",
    "# Black-box attack\n",
    "\n",
    "anon3_bb_attack = AttributeInferenceBlackBox(anon3_art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get original model's predictions\n",
    "anon3_x_train_predictions = np.array([np.argmax(arr) for arr in anon3_art_classifier.predict(data)]).reshape(-1,1)\n",
    "\n",
    "# train attack model\n",
    "anon3_bb_attack.fit(test_data)\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_anon3_bb = anon3_bb_attack.infer(x_train_for_attack, anon3_x_train_predictions, values=values)\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_anon3_bb == x_train_feature.reshape(1,-1)) / len(inferred_train_anon3_bb)\n",
    "print(train_acc)\n",
    "\n",
    "# White box attacks\n",
    "\n",
    "anon3_wb_attack = AttributeInferenceWhiteBoxLifestyleDecisionTree(anon3_art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_anon3_wb1 = anon3_wb_attack.infer(x_train_for_attack, anon3_x_train_predictions, values=values, priors=priors)\n",
    "\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_anon3_wb1 == x_train_feature.reshape(1,-1)) / len(inferred_train_anon3_wb1)\n",
    "print(train_acc)\n",
    "\n",
    "anon3_wb2_attack = AttributeInferenceWhiteBoxDecisionTree(anon3_art_classifier, attack_feature=attack_feature)\n",
    "\n",
    "# get inferred values\n",
    "inferred_train_anon3_wb2 = anon3_wb2_attack.infer(x_train_for_attack, anon3_x_train_predictions, values=values, priors=priors)\n",
    "\n",
    "# check accuracy\n",
    "train_acc = np.sum(inferred_train_anon3_wb2 == x_train_feature.reshape(1,-1)) / len(inferred_train_anon3_wb2)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.317115551694179, 0.21245634458672877)\n",
      "(1, 0.0)\n",
      "(1, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# black-box anonymized\n",
    "print(calc_precision_recall(inferred_train_anon3_bb, x_train_feature, positive_value=1.420169037))\n",
    "\n",
    "# white-box 1 anonymized\n",
    "print(calc_precision_recall(inferred_train_anon3_wb1, x_train_feature, positive_value=1.420169037))\n",
    "\n",
    "# white-box 2 anonymized\n",
    "print(calc_precision_recall(inferred_train_anon3_wb2, x_train_feature, positive_value=1.420169037))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall decreased in all cases. Accuracy of two of the attacks also decreased."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
